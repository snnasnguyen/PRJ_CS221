{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10009585,"sourceType":"datasetVersion","datasetId":6162105},{"sourceId":10014546,"sourceType":"datasetVersion","datasetId":6165612},{"sourceId":209302822,"sourceType":"kernelVersion"},{"sourceId":209507915,"sourceType":"kernelVersion"},{"sourceId":218412977,"sourceType":"kernelVersion"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T08:02:27.389540Z","iopub.execute_input":"2025-01-20T08:02:27.390030Z","iopub.status.idle":"2025-01-20T08:02:27.424911Z","shell.execute_reply.started":"2025-01-20T08:02:27.389966Z","shell.execute_reply":"2025-01-20T08:02:27.423879Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#Zero_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-chatgpt/zero_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#One_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-chatgpt/one_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:16.987325Z","iopub.status.idle":"2025-01-19T16:21:16.987864Z","shell.execute_reply.started":"2025-01-19T16:21:16.987568Z","shell.execute_reply":"2025-01-19T16:21:16.987612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#few_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-chatgpt/few_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:16.989499Z","iopub.status.idle":"2025-01-19T16:21:16.990007Z","shell.execute_reply.started":"2025-01-19T16:21:16.989764Z","shell.execute_reply":"2025-01-19T16:21:16.989791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#chain_of_thought_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-chatgpt/chain_of_thought_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:16.991431Z","iopub.status.idle":"2025-01-19T16:21:16.991927Z","shell.execute_reply.started":"2025-01-19T16:21:16.991676Z","shell.execute_reply":"2025-01-19T16:21:16.991700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Zero_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-chatgpt/zero_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:16.993481Z","iopub.status.idle":"2025-01-19T16:21:16.993912Z","shell.execute_reply.started":"2025-01-19T16:21:16.993680Z","shell.execute_reply":"2025-01-19T16:21:16.993698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#one_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-chatgpt/one_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:16.995424Z","iopub.status.idle":"2025-01-19T16:21:16.995835Z","shell.execute_reply.started":"2025-01-19T16:21:16.995616Z","shell.execute_reply":"2025-01-19T16:21:16.995634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#few_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/few-shot-prompt-test-evaluate/few_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:16.997438Z","iopub.status.idle":"2025-01-19T16:21:16.997832Z","shell.execute_reply.started":"2025-01-19T16:21:16.997628Z","shell.execute_reply":"2025-01-19T16:21:16.997646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#chain_of_thought_prompt_test_evaluate\n\nwith open('/kaggle/input/few-shot-prompt-test-evaluate/chain_of_thought_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:16.999551Z","iopub.status.idle":"2025-01-19T16:21:16.999961Z","shell.execute_reply.started":"2025-01-19T16:21:16.999774Z","shell.execute_reply":"2025-01-19T16:21:16.999798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#zero_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/zero_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:17.001525Z","iopub.status.idle":"2025-01-19T16:21:17.001929Z","shell.execute_reply.started":"2025-01-19T16:21:17.001718Z","shell.execute_reply":"2025-01-19T16:21:17.001756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#one_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/one_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:17.005023Z","iopub.status.idle":"2025-01-19T16:21:17.005412Z","shell.execute_reply.started":"2025-01-19T16:21:17.005239Z","shell.execute_reply":"2025-01-19T16:21:17.005258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#few_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/few_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:17.006373Z","iopub.status.idle":"2025-01-19T16:21:17.006833Z","shell.execute_reply.started":"2025-01-19T16:21:17.006555Z","shell.execute_reply":"2025-01-19T16:21:17.006573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#chain_of_thought_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/chain_of_thought_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:17.008132Z","iopub.status.idle":"2025-01-19T16:21:17.008480Z","shell.execute_reply.started":"2025-01-19T16:21:17.008310Z","shell.execute_reply":"2025-01-19T16:21:17.008328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#zero_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/zero_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:17.010059Z","iopub.status.idle":"2025-01-19T16:21:17.010409Z","shell.execute_reply.started":"2025-01-19T16:21:17.010242Z","shell.execute_reply":"2025-01-19T16:21:17.010259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#one_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/one_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:17.011411Z","iopub.status.idle":"2025-01-19T16:21:17.011720Z","shell.execute_reply.started":"2025-01-19T16:21:17.011566Z","shell.execute_reply":"2025-01-19T16:21:17.011582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#few_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/few_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:17.013431Z","iopub.status.idle":"2025-01-19T16:21:17.013821Z","shell.execute_reply.started":"2025-01-19T16:21:17.013619Z","shell.execute_reply":"2025-01-19T16:21:17.013636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#chain_of_thought_prompt_test_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/chain_of_thought_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:17.015114Z","iopub.status.idle":"2025-01-19T16:21:17.015463Z","shell.execute_reply.started":"2025-01-19T16:21:17.015299Z","shell.execute_reply":"2025-01-19T16:21:17.015317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#zero_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/llama-test-eva/zero_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n#one_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/llama-test-eva/one_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n#few_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/llama-test-eva/few_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n#chain_of_thought_prompt_test_evaluate\n\nwith open('/kaggle/input/llama-test-eva/chain_of_thought_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:17.017065Z","iopub.status.idle":"2025-01-19T16:21:17.017511Z","shell.execute_reply.started":"2025-01-19T16:21:17.017310Z","shell.execute_reply":"2025-01-19T16:21:17.017352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#zero_shot_prompt_dev_evaluate\nwith open('/kaggle/input/llama-test-eva/zero_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n#one_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/llama-test-eva/one_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n#few_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/llama-test-eva/few_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n#chain_of_thought_prompt_dev_evaluate\n\nwith open('/kaggle/input/llama-test-eva/chain_of_thought_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T16:21:17.019489Z","iopub.status.idle":"2025-01-19T16:21:17.019891Z","shell.execute_reply.started":"2025-01-19T16:21:17.019682Z","shell.execute_reply":"2025-01-19T16:21:17.019699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#xlm_r_dev_evaluate\nwith open('/kaggle/input/notebookc09048c531/xlm_r_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n\n\n#xlm_r_test_evaluate\nwith open('/kaggle/input/notebookc09048c531/xlm_r_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T08:02:35.759621Z","iopub.execute_input":"2025-01-20T08:02:35.760708Z","iopub.status.idle":"2025-01-20T08:02:35.908198Z","shell.execute_reply.started":"2025-01-20T08:02:35.760663Z","shell.execute_reply":"2025-01-20T08:02:35.907008Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.5286652078774617\nF1 Score: 0.7533778556745491\nEM Score: 0.5013574660633484\nF1 Score: 0.7283278794319953\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}